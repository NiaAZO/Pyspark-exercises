{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "eee3c8df24c0920c7be7faf65104d0bf", "grade": false, "grade_id": "cell-f8987996be9f1238", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# V\u00eddeos que fueron tendencia en YouTube\n\n### Disponible en Kaggle en:\nhttps://www.kaggle.com/datasnaek/youtube-new\n\n\nYouTube, el sitio web para compartir videos de fama mundial, mantiene una lista de los mejores videos de tendencias en la plataforma. Seg\u00fan la revista Variety, *Para determinar los videos m\u00e1s populares del a\u00f1o, YouTube utiliza una combinaci\u00f3n de factores que incluyen la medici\u00f3n de las interacciones de los usuarios (n\u00famero de visitas, compartidos, comentarios y me gusta). No son necesariamente los v\u00eddeos m\u00e1s vistos del a\u00f1o en general*. Los que se sit\u00faan en la parte m\u00e1s alta de la lista de tendencias de YouTube suelen ser o bien v\u00eddeos musicales (como el famoso \"Gagnam Style\"), actuaciones de *celibrities* y / o reality shows, y v\u00eddeos virales variados de una persona aleatoria, c\u00e1mara en mano.\n\nEste conjunto de datos es un registro diario de los videos m\u00e1s populares de YouTube. Incluye varios meses de datos en v\u00eddeos de tendencias diarias de YouTube. Se incluyen datos para las regiones de EEUU, Reino Unido, Dinamarca, Canad\u00e1 y Francia con hasta 200 videos de tendencias listados por d\u00eda. Aqu\u00ed solo usaremos los de EEUU, Canad\u00e1 y Reino Unido. Los datos de cada pa\u00eds est\u00e1n en un archivo separado. Las variables incluyen el t\u00edtulo del video, t\u00edtulo del canal, tiempo de publicaci\u00f3n, etiquetas, vistas, me gusta y no me gusta, descripci\u00f3n y recuento de comentarios. Se recopil\u00f3 utilizando la API de YouTube."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "9a6b4dc108ddf890c659e33701965428", "grade": false, "grade_id": "cell-f74d7bfd01811789", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Variables y significado"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "0b10e97ef91e0ae368718feaaf3c3137", "grade": false, "grade_id": "cell-9cfb34982bd4eb04", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Las variables utilizadas para describir cada v\u00eddeo son:\n\n* video_id (string) ID \u00fanico. Se ha asignado un video en la plataforma de YouTube. \n* trending_date (string) La fecha en que el video era tendencia \n* title (string) T\u00edtulo del v\u00eddeo\n* channel_title (string) T\u00edtulo del canal de publicaci\u00f3n en la categor\u00eda de plataforma\n* category_id (string) El tipo de categor\u00eda del v\u00eddeo\n* publish_time (string) La fecha de publicaci\u00f3n del v\u00eddeo \n* tags (string) Etiquetas asociadas al v\u00eddeo, separadas por |\n* views (entero) N\u00famero total de vistas en el v\u00eddeo.\n* likes (entero) N\u00famero de Me gusta en el v\u00eddeo\n* dislikes (entero) N\u00famero de No me gusta en el v\u00eddeo\n* comment_count (entero) Un recuento total de comentarios en el video \n* comments_disabled (booleano) Si los comentarios estaban desactivados (true) o activados (false) en el video \n* ratings_disabled (booleano) Si la opci\u00f3n de dar me gusta o no al video est\u00e1 deshabilitada (true), en cuyo caso el n\u00famero de  Me gusta y de No me gusta ser\u00e1 0. \n* video_error_or_removed (booleano) Si el video tiene alg\u00fan error o se elimin\u00f3 despu\u00e9s de cargar el pa\u00eds\n* description (string): Descripci\u00f3n textual"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "76dc5b331cac3113e9e77522358617bf", "grade": false, "grade_id": "cell-b4f9c37a2b92d2e6", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**INSTRUCCIONES**: en cada celda debes responder a la pregunta formulada, asegur\u00e1ndote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a `None`. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del c\u00e1lculo quede guardado exactamente en la variable que ven\u00eda inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). **No olvides borrar la l\u00ednea *raise NotImplementedError()* de cada celda cuando hayas completado la soluci\u00f3n de esa celda y quieras probarla**.\n\nDespu\u00e9s de cada celda evaluable ver\u00e1s una celda con c\u00f3digo. Ejec\u00fatala (no modifiques su c\u00f3digo) y te dir\u00e1 si tu soluci\u00f3n es correcta o no. En caso de ser correcta, se ejecutar\u00e1 correctamente y no mostrar\u00e1 nada, pero si no lo es mostrar\u00e1 un error. Adem\u00e1s de esas pruebas, se realizar\u00e1n algunas m\u00e1s (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la soluci\u00f3n correcta o no. Aseg\u00farate de que, al menos, todas las celdas indican que el c\u00f3digo es correcto antes de enviar el notebook terminado."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "5c1f80078dab9ae1c71185cfb397a81e", "grade": false, "grade_id": "cell-69ec0993eeaff3ac", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Sobre los datasets youtube_USvideos.csv, youtube_CAvideos, youtube_GBvideos.csv se pide:"}, {"cell_type": "markdown", "metadata": {}, "source": "**(1 punto)** Ejercicio 1\n\n* Leer cada uno de estos ficheros en una variable, **sin intentar** que Spark infiera el tipo de dato de cada columna\n* Puesto que existen columnas que contienen una coma enmedio del valor, en esos casos los valores vienen entre comillas dobles. Spark ya contempla esta posibilidad y puede leerlas adecuadamente **si al leer le indicamos las siguientes opciones adicionales** adem\u00e1s de las que ya sueles usar: `.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")`.\n* Aseg\u00farate de que las **filas que no tienen el formato correcto sean descartadas**, indicando tambi\u00e9n la opci\u00f3n `mode` con el valor `DROPMALFORMED` como vimos en clase.\n* Encadenadas con la operaci\u00f3n de lectura `.csv()` de cada fichero, a\u00f1adir para cada uno de los ficheros:\n  - Una transformaci\u00f3n para crear una nueva columna `pais` con una constante de tipo string (utiliza la funci\u00f3n `F.lit(\"valor\")`) indicando el pa\u00eds, que debe tomar como valores `\"EEUU\"`, `\"CA\"` y `\"GB\"` para EEUU, Canad\u00e1 y Gran Breta\u00f1a respectivamente. Dicho valor ser\u00e1 igual para todas las filas de cada uno de los tres DF, pero distinto de un DF a otro.\n  * Una transformaci\u00f3n que elimine la columna `description`, la cual contiene un texto que no analizaremos.\n* Crear finalmente un nuevo DF `videosRawDF` en el que se hayan unido los tres DF anteriores. No debe ser cacheado todav\u00eda puesto que vamos a realizar operaciones de limpieza m\u00e1s adelante, y trabajaremos a partir de entonces con otro DF resultante."}, {"cell_type": "code", "execution_count": 1, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "3b1cc0e0ae5b216c4e818d01d9378a7a", "grade": false, "grade_id": "read_csv", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- video_id: string (nullable = true)\n |-- trending_date: string (nullable = true)\n |-- title: string (nullable = true)\n |-- channel_title: string (nullable = true)\n |-- category_id: string (nullable = true)\n |-- publish_time: string (nullable = true)\n |-- tags: string (nullable = true)\n |-- views: string (nullable = true)\n |-- likes: string (nullable = true)\n |-- dislikes: string (nullable = true)\n |-- comment_count: string (nullable = true)\n |-- thumbnail_link: string (nullable = true)\n |-- comments_disabled: string (nullable = true)\n |-- ratings_disabled: string (nullable = true)\n |-- video_error_or_removed: string (nullable = true)\n |-- pais: string (nullable = false)\n\n"}], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\nfrom pyspark.sql import functions as F\n\n# YOUR CODE HERE\n\nUSDF = spark.read\\\n            .option(\"header\", \"true\")\\\n            .option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n            .option(\"mode\", \"DROPMALFORMED\")\\\n            .csv(\"gs://proyecto-ucm/datos/youtube_USvideos.csv\")\\\n                .withColumn(\"pais\", F.lit(\"EEUU\"))\\\n                .drop(\"description\")\n\nCADF = spark.read\\\n            .option(\"header\", \"true\")\\\n            .option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n            .option(\"mode\", \"DROPMALFORMED\")\\\n            .csv(\"gs://proyecto-ucm/datos/youtube_CAvideos.csv\")\\\n                .withColumn(\"pais\", F.lit(\"CA\"))\\\n                .drop(\"description\")\n        \nUKDF = spark.read\\\n            .option(\"header\", \"true\")\\\n            .option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n            .option(\"mode\", \"DROPMALFORMED\")\\\n            .csv(\"gs://proyecto-ucm/datos/youtube_GBvideos.csv\")\\\n                .withColumn(\"pais\", F.lit(\"GB\"))\\\n                .drop(\"description\")\n\nUSvideosDF = USDF\nCAvideosDF = CADF\nGBvideosDF = UKDF\nvideosRawDF = USvideosDF.union(CAvideosDF)\\\n                        .union(GBvideosDF)\nvideosRawDF.printSchema()\n"}, {"cell_type": "code", "execution_count": 2, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "4ebf649a36c3332710382e12e9b180c5", "grade": true, "grade_id": "read_csv_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "from pyspark.sql.types import DoubleType\nassert((videosRawDF.count() == 136992) | (videosRawDF.count() == 120750))\nassert(\"description\" not in videosRawDF.columns)\nassert((videosRawDF.where(\"pais = 'EEUU'\").count() == 48137) | (videosRawDF.where(\"pais = 'EEUU'\").count() == 40953))\nassert((videosRawDF.where(\"pais = 'CA'\").count() == 45560) | (videosRawDF.where(\"pais = 'CA'\").count() == 40881))\nassert((videosRawDF.where(\"pais = 'GB'\").count() == 43295) | (videosRawDF.where(\"pais = 'GB'\").count() == 38916))"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "ac2250a5429f9b4b3854c8056a46ff18", "grade": false, "grade_id": "cell-b90f5b934eda250e", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(1 punto)** Ejercicio 2\n\n* Las columnas `trending_date` y `publish_time` son en realidad de tipo fecha y de tipo timestamp (instante de tiempo), respectivamente, que Spark deber\u00eda procesar como tales. Por otro lado, `likes`, `dislikes`, `comment_count` son de tipo entero, y `comments_disabled` es de tipo booleano. La columna `category_id` tambi\u00e9n lo es pero la utilizaremos m\u00e1s adelante para reemplazarla por sus verdaderas categor\u00edas, por lo que no vamos a modificarla ahora y la mantenemos como string. Partiendo de `vidosRawDF`, **reemplaza** cada una de estas columnas por su versi\u00f3n convertida al tipo de dato correcto en cada caso, utilizando `withColumn` con el mismo nombre de la columna existente. Para las columnas de tipo fecha y timestamp, el nuevo valor de la columna viene dado por el siguiente c\u00f3digo:\n\n        F.to_date(\"colName\", \"yy.dd.MM\") # para fechas\n        F.from_unixtime(F.unix_timestamp('colName', \"yyyy-MM-dd'T'HH:mm:ss\")).cast(\"timestamp\") # para timestamp\n\n* Despu\u00e9s de las conversiones, eliminar todas las filas que tengan alg\u00fan valor nulo\n* El DF resultante de todas estas operaciones debe quedar almacenado en la variable `videosDF`, **cacheado**.\n"}, {"cell_type": "code", "execution_count": 3, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "7cb06ef0eb82ebb70d1528083de619d2", "grade": false, "grade_id": "convert_timestamp", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"data": {"text/plain": "DataFrame[video_id: string, trending_date: date, title: string, channel_title: string, category_id: string, publish_time: timestamp, tags: string, views: string, likes: int, dislikes: int, comment_count: int, thumbnail_link: string, comments_disabled: boolean, ratings_disabled: string, video_error_or_removed: string, pais: string]"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "# No olvides los imports que necesites...\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType, BooleanType, DateType, TimestampType\n\n# L\u00cdNEAS EVALUABLES, NO RENOMBRAR LAS VARIABLES\nvideosDF = videosRawDF.withColumn(\"likes\", F.col(\"likes\").cast(IntegerType()))\\\n                      .withColumn(\"dislikes\", F.col(\"dislikes\").cast(IntegerType()))\\\n                      .withColumn(\"comment_count\", F.col(\"comment_count\").cast(IntegerType()))\\\n                      .withColumn(\"comments_disabled\", F.col(\"comments_disabled\").cast(BooleanType()))\\\n                      .withColumn(\"trending_date\", F.to_date(\"trending_date\", \"yy.dd.MM\"))\\\n                      .withColumn(\"publish_time\", F.from_unixtime(F.unix_timestamp('publish_time', \"yyyy-MM-dd'T'HH:mm:ss\")).cast(\"timestamp\"))\\\n                      .dropna()\nvideosDF.cache()\n# YOUR CODE HERE\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "7a5e059625e1080585a339fbe73a68cf", "grade": true, "grade_id": "convert_timestamp_tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "dtypes = dict(videosDF.dtypes)\nassert((videosDF.count() == 120739) | (videosDF.count() == 120746))\nassert(dtypes[\"publish_time\"] == \"timestamp\")\nassert(dtypes[\"trending_date\"] == \"date\")\nassert(dtypes[\"likes\"] == \"int\")\nassert(dtypes[\"dislikes\"] == \"int\")\nassert(dtypes[\"comment_count\"] == \"int\")\nassert(dtypes[\"comments_disabled\"] == \"boolean\")\nassert(videosDF.is_cached)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "8587dc30ffa23836889b4093a7fe5647", "grade": false, "grade_id": "cell-fc88821f19453a51", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(2 puntos)** Ejercicio 3\n\nPartiendo de `videosDF`:\n\n* Traduce la categor\u00eda a un string utilizando la equivalencia indicada en el diccionario de la celda siguiente. Para ello puedes usar directamente la funci\u00f3n `videosDF.replace(to_replace=diccionario, subset=['category_id'])` **que se aplica al DF completo** y devuelve un DF completamente nuevo en el que ha realizado en las columnas especificadas en la lista `subset` los reemplazamientos que le hayamos indicado en el diccionario `to_replace`. Por tanto, esta funci\u00f3n **no se utiliza en combinaci\u00f3n con `withColumn`** sino que debe ir fuera de la secuencia de transformaciones encadenadas, por ejemplo al principio de todo, y su resultado debe almacenarse en la variable `replacedCategoryDF`.\n\nA continuaci\u00f3n, partiendo de `replacedCategoryDF`: \n* A\u00f1ade una nueva columna llamada `dias_hasta_viral` que contenga el n\u00famero de d\u00edas que han pasado entre la fecha en que se public\u00f3 un v\u00eddeo y el instante en el que se hizo viral. Para ello, utiliza `withColumn` en combinaci\u00f3n con la funci\u00f3n `F.datediff(\"columnaFuturo\", \"columnaPasado\")`\n* A\u00f1ade otra nueva columna llamada `diasemana` que contenga el d\u00eda de la semana en el que se ha publicado cada v\u00eddeo. Puedes usar la funci\u00f3n `F.dayofweek(\"colName\")` en combinaci\u00f3n con `withColumn`.\n* Vamos a empezar a utilizar la columna `tags`, para lo que necesitamos hacerla \"usable\". Encadenaremos estas tres transformaciones:\n  - El primer paso consiste en convertir todas las palabras en min\u00fasculas, para que exista m\u00e1s coincidencia entre etiquetas y podamos ver que la misma aparece en varios v\u00eddeos. Puedes usar la funci\u00f3n `F.lower(F.col(\"columnName\"))` en combinaci\u00f3n con `withColumn` sobre la columna original. Atenci\u00f3n: la funci\u00f3n `lower` no admite directament el nombre de columna sino un objeto columna.\n  - El segundo paso ser\u00e1 eliminar el car\u00e1cter `\"` que rodea a la mayor\u00eda de t\u00e9rminos individuales, puesto que no lo necesitamos ya que el separador entre t\u00e9rminos es `|`. Puedes usar la funci\u00f3n `F.regexp_replace(\"columnName\", \"\\\"\", \"\")` en combinaci\u00f3n con `withColumn` sobre la salida del apartado anterior, o incluso simplemente envolver a `F.lower(...)` con `regexp_replace`: `F.regexp_replace(F.lower(...), \"\\\"\", \"\")`.\n  - El tercer paso consiste en convertirla en una columna de tipo **vector** de palabras, dividiendo la cadena de texto por el separador `|` que debe ser escapado poniendo una barra `\\` delante porque `|` se utiliza tambi\u00e9n en expresiones regulares. **Reemplaza** la columna `tags` por el resultado de aplicarle la funci\u00f3n `F.split(\"colName\", \"\\|\")`. Dicha funci\u00f3n debe ser utilizada dentro de `withColumn`.\n* El resultado debe quedar guardado en la variable `videosExtraInfoDF`."}, {"cell_type": "code", "execution_count": 5, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "40913b4ab429e96be8be9d52bca1d398", "grade": false, "grade_id": "aniade_tiempos", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"data": {"text/plain": "DataFrame[video_id: string, trending_date: date, title: string, channel_title: string, category_id: string, publish_time: timestamp, tags: array<string>, views: string, likes: int, dislikes: int, comment_count: int, thumbnail_link: string, comments_disabled: boolean, ratings_disabled: string, video_error_or_removed: string, pais: string, dias_hasta_viral: int, diasemana: int]"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR VARIABLES\n# imports......\nfrom pyspark.sql import functions as F\n\ndiccionario = {\n   \"1\": \"Film & Animation\",\n   \"2\": \"Autos & Vehicles\",\n   \"10\": \"Music\",\n   \"15\": \"Pets & Animals\",\n   \"17\": \"Sports\",\n   \"18\": \"Short Movies\",\n   \"19\": \"Travel & Events\",\n   \"20\": \"Gaming\",\n   \"21\": \"Videoblogging\",\n   \"22\": \"People & Blogs\",\n   \"23\": \"Comedy\",\n   \"24\": \"Entertainment\",\n   \"25\": \"News & Politics\",\n   \"26\": \"Howto & Style\",\n   \"27\": \"Education\",\n   \"28\": \"Science & Technology\",\n   \"29\": \"Nonprofits & Activism\",\n   \"30\": \"Movies\",\n   \"31\": \"Anime/Animation\",\n   \"32\": \"Action/Adventure\",\n   \"33\": \"Classics\",\n   \"34\": \"Comedy\",\n   \"35\": \"Documentary\",\n   \"36\": \"Drama\",\n   \"37\": \"Family\",\n   \"38\": \"Foreign\",\n   \"39\": \"Horror\",\n   \"40\": \"Sci-Fi/Fantasy\",\n   \"41\": \"Thriller\",\n   \"42\": \"Shorts\",\n   \"43\": \"Shows\",\n   \"44\": \"Trailers\"\n}\n# YOUR CODE HERE\nreplacedCategoryDF = videosDF.replace(to_replace=diccionario, subset=['category_id'])\nvideosExtraInfoDF = replacedCategoryDF\\\n                    .withColumn(\"dias_hasta_viral\",F.datediff(\"trending_date\", \"publish_time\"))\\\n                    .withColumn(\"diasemana\",  F.dayofweek(\"publish_time\"))\\\n                    .withColumn(\"tags\", F.lower(F.col(\"tags\")))\\\n                    .withColumn(\"tags\", F.regexp_replace(\"tags\", \"\\\"\", \"\"))\\\n                    .withColumn(\"tags\", F.split(\"tags\", \"\\|\"))\n\nvideosExtraInfoDF.cache()"}, {"cell_type": "code", "execution_count": 6, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "ff264b92eb9345a26ec9470c4d3324db", "grade": true, "grade_id": "aniade_tiempos_test", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "dtypesExtraInfo = dict(videosExtraInfoDF.dtypes)\nassert(dtypesExtraInfo[\"category_id\"] == \"string\")\nassert(dtypesExtraInfo[\"tags\"] == \"array<string>\")\nassert(dtypesExtraInfo[\"diasemana\"] == \"int\")\nassert(dtypesExtraInfo[\"dias_hasta_viral\"] == \"int\")\nassert(videosExtraInfoDF.where(\"category_id = 'Education'\").distinct().count() == 3103)\nr = videosExtraInfoDF.select(\"video_id\", \"category_id\", \"diasemana\", \"tags\").where(\"video_id == 'YVfyYrEmzgM'\").head()\nassert(r.category_id == \"Education\")\nassert(r.diasemana == 2)\nassert((\"ted\" in r.tags) & (\"ted-ed\" in r.tags) & (\"ted education\" in r.tags))"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "714aa44689924e527df8f805f4c3f65f", "grade": false, "grade_id": "cell-a71a6b17b1e0d613", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(3 puntos)** Ejercicio 4\n\nPartiendo de `videosExtraInfoDF`:\n\n* A\u00f1adir una nueva columna `dias_viral_pais` que contenga el **n\u00famero de d\u00edas durante los cuales un v\u00eddeo ha sido tendencia en cada pa\u00eds**. El dataset original contiene en muchos casos varias filas para un mismo v\u00eddeo, incluso el mismo v\u00eddeo (mismo identificador) puede encontrarse en distintos pa\u00edses, variando en la columna relativa al d\u00eda en que el v\u00eddeo es tendencia. Lo que se pide es que la nueva columna contenga, **para cada fila**, la **diferencia** en d\u00edas entre la fecha **m\u00ednima y m\u00e1xima** que ese v\u00eddeo ha sido tendencia **en ese pa\u00eds**. El valor en ambas columnas (m\u00ednima y m\u00e1xima) se repetir\u00e1 para todas las filas de cada v\u00eddeo y pa\u00eds, pero ser\u00e1 distinto entre v\u00eddeos con distinto identificador y/o distinto pa\u00eds. Primero deben calcularse una columna para el m\u00ednimo y otra para el m\u00e1ximo por cada v\u00eddeo y pa\u00eds, **mediante agregaciones en una ventana (sin ordenar). No se debe utilizar la operaci\u00f3n JOIN**. Una vez calculadas ambas columnas, la columna `dias_viral_pais` ser\u00e1 el resultado de aplicarles la funci\u00f3n `F.datediff` utilizada anteriormente, y tras ello, se deben eliminar las columnas de m\u00ednimo y m\u00e1ximo (si se hubieran llegado a crear) que ya no son necesarias.\n\n* Tras esto, eliminar las filas duplicadas, atendiendo exclusivamente a las columnas `video_id` y `pais`, puesto que la informaci\u00f3n de qu\u00e9 d\u00edas fue tendencia ya la habremos resumido en la columna `dias_viral_pais`, y no queremos que cada v\u00eddeo aparezca varias veces y pueda falsear los res\u00famenes que haremos justo despu\u00e9s. \n\n* El resultado de estos dos apartados debe almacenarse en la variable `videosDiasViralDF`, que **debe ser cacheada** porque  usaremos este DF varias veces m\u00e1s adelante.\n\n\u00bfSon todas las categor\u00edas igual de viralizantes? \u00bfAdem\u00e1s de la categor\u00eda, es distinto el comportamiento seg\u00fan el pa\u00eds?\n\nA continuaci\u00f3n, y partiendo de `videosDiasViralDF`:\n\n* Calcular un nuevo DF llamado `diasViralCategoriaPaisDF` con tantas filas como **categor\u00edas** y con tantas columnas como **pa\u00edses** (es decir, 3 m\u00e1s la columna de categor\u00edas), que contenga en cada celda el n\u00famero **medio** de d\u00edas que un v\u00eddeo de cada categor\u00eda permanece siendo viral en ese pa\u00eds. PISTA: Utilizar la funci\u00f3n **pivot**.\n\n* Por \u00faltimo, calcular otro DF llamado `videosPorDiasemanaDF` con tantas filas como **categor\u00edas** y tantas columnas como **d\u00edas de la semana**, de forma que cada celda contenga el **n\u00famero de v\u00eddeos DISTINTOS** que se han publicado en esa categor\u00eda ese d\u00eda de la semana. Por v\u00eddeos distintos se entiende que **no** debemos contar varias veces un mismo v\u00eddeo (mismo `video_id`) ni siquiera si se ha publicado en distinto pa\u00eds. Tras el c\u00e1lculo, **renombrar las columnas** para que sus nombres sean \"Lunes\", \"Martes\", \"Mi\u00e9rcoles\", \"Jueves\", \"Viernes\", \"S\u00e1bado\", \"Domingo\" en lugar de los n\u00fameros enteros del 1 al 7 (atenci\u00f3n a las tildes). PISTA: piensa bien la funci\u00f3n de agregaci\u00f3n necesaria, y la columna sobre la que debe aplicarse."}, {"cell_type": "code", "execution_count": 7, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "1b2b2679bd6bb585f46a68722dccecb4", "grade": false, "grade_id": "categoria_pais_semana", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR VARIABLES\n# import.....\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\n# YOUR CODE HERE\n\nventanaVideoId = Window().partitionBy(\"video_id\", \"pais\")\nvideosDiasViralDF = videosExtraInfoDF.withColumn(\"dias_minimo\",F.min(\"trending_date\").over(ventanaVideoId))\\\n                                     .withColumn(\"dias_maximo\",F.max(\"trending_date\").over(ventanaVideoId))\\\n                                     .withColumn(\"dias_viral_pais\",F.datediff(\"dias_maximo\", \"dias_minimo\"))\\\n                                     .drop(\"dias_minimo\", \"dias_maximo\")\\\n                                     .dropDuplicates([\"video_id\", \"pais\"])\nvideosDiasViralDF.cache()\nvideosDiasViralDF.count()\n\n\n\ndiasViralCategoriaPaisDF = videosDiasViralDF.groupBy(\"category_id\")\\\n                                            .pivot(\"pais\").agg(F.mean(\"dias_viral_pais\"))\n                                        \n\n\n\nvideosPorDiasemanaDF = videosDiasViralDF.groupBy(\"category_id\")\\\n                                        .pivot(\"diasemana\").agg(F.countDistinct(\"video_id\"))\\\n                                        .withColumnRenamed(\"1\", \"Lunes\")\\\n                                        .withColumnRenamed(\"2\", \"Martes\")\\\n                                        .withColumnRenamed(\"3\", \"Mi\u00e9rcoles\")\\\n                                        .withColumnRenamed(\"4\", \"Jueves\")\\\n                                        .withColumnRenamed(\"5\", \"Viernes\")\\\n                                        .withColumnRenamed(\"6\", \"S\u00e1bado\")\\\n                                        .withColumnRenamed(\"7\", \"Domingo\")\n                                       \n\n\n\n"}, {"cell_type": "code", "execution_count": 8, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "af67605ef5a72cd9a503158113c27a81", "grade": true, "grade_id": "categoria_pais_semana_test", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(videosDiasViralDF.is_cached)\nassert(dict(videosDiasViralDF.dtypes)[\"dias_viral_pais\"] == \"int\")\nassert((videosDiasViralDF.count() == 34045) | (videosDiasViralDF.count() == 34050))\ndiasViralCategoriaPais = diasViralCategoriaPaisDF.sort(\"category_id\").collect()\nassert(diasViralCategoriaPais[0].category_id == \"Autos & Vehicles\")\nassert(round(diasViralCategoriaPais[0].CA, 3) == 0.431)\nassert(diasViralCategoriaPais[3].category_id == \"Entertainment\")\nassert(round(diasViralCategoriaPais[3].CA, 3) == 0.637)\nassert(\"Lunes\" in videosPorDiasemanaDF.columns)\nvideosPorDiasemana = videosPorDiasemanaDF.sort(\"category_id\").collect()\nassert(videosPorDiasemana[0].Domingo == 18)\nassert(videosPorDiasemana[2].category_id == \"Education\")\nassert(videosPorDiasemana[2].Lunes == 108)\nassert(videosPorDiasemana[2].S\u00e1bado == 82)\nassert(videosDiasViralDF.where(\"video_id = 'f6Egj7ncOi8' and pais = 'CA'\").head().dias_viral_pais == 0)\nassert(videosDiasViralDF.where(\"video_id = 'f6Egj7ncOi8' and pais = 'GB'\").head().dias_viral_pais == 15)\nassert(videosDiasViralDF.where(\"video_id = 'f6Egj7ncOi8' and pais = 'EEUU'\").head().dias_viral_pais == 6)\nassert(\"Martes\" in videosPorDiasemanaDF.columns)\nassert(\"Mi\u00e9rcoles\" in videosPorDiasemanaDF.columns)\nassert(\"Jueves\" in videosPorDiasemanaDF.columns)\nassert(\"Viernes\" in videosPorDiasemanaDF.columns)\nassert(\"S\u00e1bado\" in videosPorDiasemanaDF.columns)\nassert(\"Domingo\" in videosPorDiasemanaDF.columns)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "fae4edd71d48e509be1f633413181e90", "grade": false, "grade_id": "cell-c5ec05706eccd480", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(3 puntos)** Ejercicio 5\n\nPartiendo de `videosDiasViralDF`, donde cada v\u00eddeo solo aparece una vez, a\u00f1adir las siguientes columnas:\n\n* Una columna entera `ocurrencias_music` con el **n\u00famero de ocurrencias** de la cadena **\"music\"** como **subcadena de cualquiera de los tags**. No es necesario que el tag sea exactamente igual a \"music\" sino que lo contenga como subcadena (recordemos que el DF de partida ya tiene todos los tags pasados a min\u00fascula). Para ello, implementa una **UDF** `subtag_music_UDF` que devuelva `IntegerType()` y que envuelva a una funci\u00f3n convencional de python llamada `subcadena_en_vector(tags)`. Esta \u00faltima debe recibir como argumento una lista de strings, y comprobar cu\u00e1ntos elementos del vector contienen como subcadena a la palabra \"music\". Prueba su funcionamiento con la lista de tags `[\"a life in music\", \"music for life\", \"bso\", \"hans zimmer\"]` que deber\u00eda devolver 2. Finalmente invoca a `subtag_music_UDF` dentro de `withColumn` sobre la columna `tags`.\n\n* Una columna `ocurrencia_media` de n\u00fameros reales con el n\u00famero **medio** de apariciones de la palabra \"music\" como subcadena de tags en los v\u00eddeos similares al de la fila actual, entendiendo similares como aquellos del **mismo pa\u00eds y la misma categor\u00eda**. No debe utilizarse JOIN sino funciones de ventana.\n\n* Una nueva columna `diff_porcentaje` que indique, en tanto por ciento, en qu\u00e9 medida el n\u00famero de aparticiones de \"music\" est\u00e1 por encima o por debajo de la media de los v\u00eddeos de su misma categor\u00eda y pa\u00eds. Debe calcularse mediante **operaciones aritm\u00e9ticas entre columnas**, sin usar la funci\u00f3n `F.when`, como la diferencia entre el n\u00famero de aparticiones en el v\u00eddeo actual menos las apariciones medias de su pa\u00eds y categor\u00eda, dividido entre este \u00faltimo y multiplicado por 100.\n\nEl resultado de todas estas transformaciones debe quedar almacenado en la variable `videosOcurrenciasMusicDF`."}, {"cell_type": "code", "execution_count": 33, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "60ece335cbbf1e5f177790021d4de04f", "grade": false, "grade_id": "ventana", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR VARIABLES\n# imports necesarios....\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\n\n\n# subtag_music = [\"a life in music\", \"music for life\", \"bso\", \"hans zimmer\"]\n\ndef subcadena_en_vector(tags):\n    return (sum([1 for c in tags if \"music\" in c]))\n\n# subcadena_en_vector(subtag_music)\n\nsubtag_music_UDF = F.udf(subcadena_en_vector, T.IntegerType()) \n\n\nventanaCategoriaPais = Window().partitionBy(\"category_id\", \"pais\")\n\nvideosOcurrenciasMusicDF = videosDiasViralDF.withColumn(\"ocurrencias_music\", subtag_music_UDF(F.col(\"tags\")))\\\n                                            .withColumn(\"ocurrencia_media\", F.mean(\"ocurrencias_music\").over(ventanaCategoriaPais))\\\n                                            .withColumn(\"diff_porcentaje\", (F.col(\"ocurrencias_music\")- F.col(\"ocurrencia_media\")) / (F.col(\"ocurrencia_media\")*100))\n              \n                                                      \n                                                        "}, {"cell_type": "code", "execution_count": 34, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "377cef7708e3316c9047eedc4ec8d7bc", "grade": true, "grade_id": "ventana_test", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(subcadena_en_vector([\"a life in music\", \"music for life\", \"bso\", \"hans zimmer\"]) == 2)\nr = videosOcurrenciasMusicDF.where(\"video_id = 'uSVW0aJdn9o'\").head()\nassert(r.ocurrencias_music == 2)\nassert(r.ocurrencia_media - 1.0172278778386845 < 0.0001)\nassert(r.diff_porcentaje - 114.54046639231825 < 0.0001)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}
